{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lecture we had learned what it is containers and what they made for. But let's look more closer to array. It stores elements in order it received them, but what if we want sort them. For example sort files in folder in alphabet order, or checks by summ in cafe database. So, we need to solve this task.\n",
    "\n",
    "Let's remember, what we can do with the array. We have O(1) access to element and... that is all. Because we already have a number of elements we don't need use dynamic array, therefor we will not pay attention to ist functionality. So, one of the main point in sorting that we should compare elements inside array to understand, are they on right places or not.\n",
    "\n",
    "`[1,2,4,5,3,2]` - here, in this array you can tell right away that 4 and 5 not in right places and should be move father to the end. `[1,2,3,2,4,5]` - still bad, we need to swap 3 and 2.`[1,2,2,3,4,5]` - yes, that is right. So, there we use two type of operations, swap and compare. Almost all sorting algorithm work on such scheme. In addition above we made some steps to first, one of the simplest sorting algorithm calling \"bubble\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "But before we move too it we should speak a little about one inportant sort algorithms classification - `Stability`. It means that the sorting algorithm keeps the order of equal elements. For example array: \\[1,3,4,`1`,`4`\\]. If soring algorithm is stable we get \\[1,`1`,3,4,`4`\\]. But, if not then we get \\[`1`,1,3,4,`4`\\]. How you can see usual '1' and colored one switched places, but in original array colored one was stayed latter than usual one.\n",
    "\n",
    "In some situation algorithm stability can be very important. For example when we sort data with two different params (sort checks firstly by date and second by summ inside one date). Another example is counting sort. If we make it stable we can use it in some more compex sort algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from the name of algorithm we can guess that we need some bubble. And no, not the ones you blow on streets. Actualy this algortihm use the idea of underwater air bubble, which pop up to surface. The lighter bubble, the higer it will pop up. In the same way the smaller the element the closer to the beginning it will pop up. Let's try\n",
    "\n",
    "`[3,2,1]` - we start move from beginning of array and in every steps we compare current and next elements. So, 3 is bigger then 2, that mean we swap 3 and 2. `[2,3,1]` - now compare 3 and 1. 3 is bigger - swap. `[2,1,3]` - ok, we finish first pass and now let's from beginning of array again.`[2,1,3]` - 2 is bigger then 1, swap. `[1,2,3]` - 2 is smaller than 3, so don't tuch them. The second pass is over and how we can see, array is sorting already. Actualy, the number of passes should be N-1, because we don't need search last incorrect position - it alreday solevd at previous step (swap work for two elements).\n",
    "\n",
    "![bubble](images/bubble.png)\n",
    "\n",
    "The complexity of such algorithm summ from 2 components - the number of passes and number of step through array in one pass. The number of passes not more than N-1 as we see above. The number of steps in one pass is the lenght of array, so it N-1 to (still, last element we don't need to move). In sum we get N-1 passes with N-1 steps each one -> (N-1)*(N-1) ~ O($N^2$). Not bad, but not good either.\n",
    "\n",
    "And what about stability? let's take a look. Bubble swap only adjacent elements and when it meets equal element, it skips them and goes on. So, the equal elements stay where they are and do not mix. That mean that bubble sorting is `stable`. \n",
    "\n",
    "There are two improvments for bubble sort. First is check the number of swap on each pass and if it equally to 0 that mean all elements already at right places and array already sorted. Second that after each pass we don't need with bubbles that already pop up. Mean after each pass biggest (or smallest, depends on parametrs) element of the remainig is at its place in ordered array (number 3 in example above). So we don't need to work with them any further. Now, every pass will take (N-k) where k - pass number. This improvments doesn't help to change \"bubble\" asymptotics to more faster, but it help speed up it a little and nod sort already sorted array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,5,7,3,2]\n",
    "for j in range (len(a) - 1):\n",
    "    for i in range (len(a) - 1):\n",
    "        if a[i] > a[i+1]:\n",
    "            a[i],a[i+1] = a[i+1],a[i]\n",
    "            pass\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide and rule(c). This phrase can probably be applied to all subsequent algorithms. But for Merge sort it works best.\n",
    "\n",
    "Main idea of Merge sort is divide array into number of parts. Each part contain two elements and after comparing them they either swap or not. After we sort elements inside each parts we merge them. Two adjacent parts merge into one part according to some rules. Let's look to merge closer:\n",
    "\n",
    "\\[2,4,5,7\\] and \\[1,2,3,6\\]. Firstly we create two pointers, one for begining of each part. Then we compare elements under pointers and take into new parts less element. Pointer in array where we take element move further \\[1\\] \\[`2`,4,5,7\\] \\[1,`2`,3,6\\]. Next, repeat operation - compare elements, see that they are equall so we take from first array. Move pointer \\[1,2\\] \\[2,`4`,5,7\\] \\[1,`2`,3,6\\]. Again repeat operation - 2 less than 4, so we take it. \\[1,2,2\\] \\[2,`4`,5,7\\] \\[1,2,`3`,6\\] -> \\[1,2,2,3\\] \\[2,`4`,5,7\\] \\[1,2,3,`6`\\] -> \\[1,2,2,3,4\\] \\[2,4,`5`,7\\]\\[1,2,3,`6`\\] -> \\[1,2,2,3,4,5\\] \\[2,4,5,`7`\\] \\[1,2,3,`6`\\] -> \\[1,2,2,3,4,5,6,7\\]. And in final we get a sorted part from two smaller parts. Note that small parts are always already sorted in previous steps. Exactly it help us merge them in O(n) operation, where n - lenght of new parts. It's not hard to guess that in the end we take the last biggest part - sorted array.\n",
    "\n",
    "![merge](images/merge.png)\n",
    "\n",
    "Now let's calculate complexity of merge sorting. Firstly, at every step of merge we work with k parts, each one take O(n), where n - length of part at the moment. So total operations number is O(n*k) = O(N), where N - array size, because all parts is parts of array. But how many steps we need for this, let's calculate. Each step size of parts doubles. At the begining parts contain 2 elements, than 4 elements, than 8 elements. So after i steps their size will be 2^i (2*2*2 i times). And we need equate 2^i to N to find i: $$N=2^i\\$$  $$log_{2}N = i\\$$\n",
    "\n",
    "Ok, we need made logN steps, each step take from us O(N) operations. In summ we get O(NlogN). And this correct for every situation regardless of the state of the array (mean the best and worth situation still have the same asymptotic). But be careful and don't forget about memory. Each of parts take some memory, and actualy we spend additional O(N) memory for it (to keep in memeory new parts when merging). For comparison 'Bubble' doesn't require additional memory from us.\n",
    "\n",
    "Stability - let's find the moments where merge sort can meet equal elements and what it do with they. As we see above, it made comparison only in merge step. So, when it meet two same element, we just tell it firstly take element from first part and then from second. In that situation the order of the same elements doesn't changes because first part is more closer to begining of array, and when we take element from it firstly we take firstly element that stay closer to begining and pu it to place that stay closer to begining of array. Order has been preserved. That means it's `stable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QSort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It one of the most famous algorithm and it has a lot modification because a lot of parameters inside it, that controlling it behavior. Let's get to the point.\n",
    "\n",
    "Main idea of Qsort... is divide array into parts and sort them independently. However, unlike merge sort this algorithm doesn't work from small parts to large one, but from large to small. As always, we have some number of iteration with some steps inside each of them. At every iteration we take one of the parts that we have (for example let's take the hole array) \\[4,3,5,4,2,1\\]. Inside of part we choose the main element (pivot) with some rules. Usualy you can just take a median. So, let's take 6 and sort this part into two other, to the left we send smaller or equall elements, to right large elements. \\[4,3,2,1,`4`\\] \\[5\\]. Now repeat this operation for each part, the parts containing one or two elements are simply sorted - \\[1,`2`\\] \\[4,3,4\\] \\[5\\] -> \\[1,2\\] \\[`3`\\] \\[4,4\\] \\[5\\]. That is all, now just forget about borders (yes, we made all operations inside our array, just remember the boders of parts) and we get sorted array - \\[1,2,3,4,4,5\\].\n",
    "\n",
    "![qsort](images/qsort.png)\n",
    "\n",
    "Asymptotics: Let's calculate how many operations algorithm takes to solve the task. Firstly, how always, each step QSort work with almost the entire array, like in merge sort, because at every step it work with the parts of array, which together make up the entire array. The complexity on one part is `O(k)`, where k - length of part. In summ as always `O(n*k)` ~ `O(N)`. However, there may be some exceptions when one of the part become shorter than other and it will be solved early. Mean, that in some step amount of elements will decreas. Sounds good, but actualy it's not so good because the more elements in part we get the more steps we need to sort it. For example if we divide array into \\[4,3,2,1\\] \\[5,6\\] or \\[3,2,1\\] \\[4,5,6\\]. In second situation we need only a one more step to sort array. In first situation there can be one step or two, if we divide first part into \\[3,2,1\\] \\[4\\]. But every step take from us O(N) operation - quite expensive. At worst situation QSort in every step divide array into parts with len 1 and N-1 like \\[3,4,5,2,1\\] \\[6\\] -> \\[3,4,2,1\\] \\[5\\] \\[6\\] -> \\[2,3,1\\] \\[4\\]\\[5\\]\\[6\\] ->->->\\[1\\]\\[2\\]\\[3\\]\\[4\\]\\[5\\]\\[6\\]. Sounds familiar doesn't it? This is our old friend Bubble sort with asymptotics $O(N^2)$. As we see it happens because at every step we choose worst pivot element which is in fact the biggest element in part.\n",
    "\n",
    "Ok, so the asymptotics of QSort is O($N^2$)? No, naturaly it very unlikely to take at every step the biggest element unless the array is specially matched for the worst case. In average it work $O(NlogN)$ steps. Plus, there are some optimisations for QSort among which you can find diffrent rules for selecting a pivot element. Some most famous rules:\n",
    "\n",
    "1. Middle,beginig or end of array - just take element with index N/2,0 or N-1\n",
    "2. The median of three - take the first, last and middle element from array and take from them the middle by value(median) element.\n",
    "3. Random - take a random element from array\n",
    "4. The median of medians - divide array into some parts (10 element in each for example) then take from each one the median and then from taken elements take median too. This rule give us garanty that array almost at every step will be divide into two parts with same length, but the search median algorithm asymptotics is O(N) (talking about it is too long, just belive) so our asymptotics become $O((N+N)logN)$ which equall to $O(NlogN)$, but the constant under O is become too big.\n",
    "\n",
    "So what the point if it can work slower than merge sort? Actually, with some small modifications, it doesn't need additional memory mean it work inside original array without creation of new one. To achieve this we need modified the divide rule. Instead of creation of two parts we just create two pointer at the begining and end of array that began move to each other and swap elements if left one bigger than right one. To be more precise, we move pointer till we find elements that bigger than pivot in left part and less than pointer in left part and then swap them. For example \\[1,2,6,3\\] \\[4,7,8,10\\] pivot 5, so in left part we move from 1 to 6 because 6 is bigger than 5. In right part we move from 10 to 4 beacuse 4 is less than 5, and then swap them. \\[1,2,`6`,3\\] \\[`4`,7,8,10\\] -> \\[1,2,`4`,3\\] \\[`6`,7,8,10\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def qSort (array, leftBorder, rightBorder):\n",
    "    lB, rB = leftBorder, rightBorder\n",
    "    if lB >= rB:\n",
    "        return\n",
    "    else:\n",
    "        pivot = array [random.randint (lB,rB)]\n",
    "        i, j = lB, rB\n",
    "        while i<=j:\n",
    "            if array[i] <= pivot:\n",
    "                i = i+1\n",
    "                continue\n",
    "            if array[j] > pivot:\n",
    "                j = j-1\n",
    "                continue\n",
    "            \n",
    "            array[i], array[j] = array[j], array[i]\n",
    "            pass\n",
    "        while (array[j] == pivot) and (j>lB): j=j-1\n",
    "        qSort (array, lB, j)\n",
    "        qSort (array, i, rB)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 7, 7, 8, 8, 43, 435]\n"
     ]
    }
   ],
   "source": [
    "a = [5,4,3,6,8,7,435,4,5,6,43,1,4,6,7,8,4]\n",
    "qSort (a, 0, len(a)-1)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability - QSort compare elements when two pointer move through array. And firstly they compare them with pivot and only after between each other. So, because equal elements can't be bigger and less than pivot at the same time QSort not compare them. But what happen, when we have some equal elements? Let's take a look. \\[8,4,3,5,6,3,`1`,2,1\\] - array, '6' - pivot, for example. After splitting array into two parts we get \\[1,4,5,6,3,'1',2\\] \\[8\\]. How we can see, QSort send 'usual' one to begining and break the order of ones. So this example should be enough to show that QSort is `not stable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting and radix sort\n",
    "\n",
    "Now let's take a look to more intersting sorting algorithm type - algorithm without comparisons. What does it mean and how can we sort elements if we even not compare them? The answer - counting them. So, main idea of counting sort is count elements. We just create another one array, that contain M elements, where M = max element in original array. And in new array for every element we meet we made +1. Lets call original array A and new one C. For every element i from A we make C\\[i\\] += 1 (increase by one). After we have gone through the whole array we clear it and are begining to go through C array. Every time we meet not a zero value into element, we put in the array A (that we have cleared before) C\\[i\\] elements i. In other worlds, we just count numbers of each elements inside A and now we know how many of each elements we need to write. After that we just write them inside A in needed order.\n",
    "\n",
    "![counting](images\\countingSort.png) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In example above we just see that we have two 'one', one 'two', two 'five' and one 'seven'. After that we just write 1,1 (two 'one'),2(one 'two'),5,5(two 'five'),7(one 'seven') and we got sorted array.\n",
    "\n",
    "So, now lets move to asymptotics. Firstly - time. To calc elements we use another one array of size M, where M - the biggest element from original array. So, we need create it, fill it and then walk throug it. And we got O(M + N + M) ~ O(N + M). Accordingly, if the number of diffrent elements is not large and the array itself is quite large we got complexity about O(N). But, if array is not big and we have elements values much bigger than array length than we got O(M), where M much bigger than N. Therefore when you want to use this algorithm directly you need to calculate if it will be useful or not.\n",
    "\n",
    "\n",
    "But what is interesting counting algorithm very rarely is used in direct way. More offten it's used as part of some others sorting algoritms. For example `radix sort`. But before we move to them, we need to look closer at stability of counting sort, because we actually need it. You will understand why a bit later.\n",
    "\n",
    "So counting sort not compare elements. Actually, in some way it forgets unique characteristics of elements and just count it. So, if we have not only value parametr in elements but some 'color' too, the algorithm would erase this data when writing it to a counting array and intead of two diffrent color of one value we would see two same elements. So not, in basic implementation this algorithmis `not stable`. But we can make it stable.\n",
    "\n",
    "To do this, we need to try not forget original array and use the order of same elements from it. To do this, we can walk through the original array with the knowledge of amounts of elements in it and use this information to create a new one - sorted array. But if we have only the knowledge abount amount of elements it isn't help us, bacause we don't know where current element (in original array) should stay. So, can we instead of amounts of elements write the positions of elements where they should be in sorted array? Yes, why not. To do this we need after calculating the counting array add dummy 0 to the beginning of array (mean the first element should stand at 0 position) and go through it with special function b\\[i\\] = b\\[i\\] + b\\[i-1\\]. That mean that at each step we calculate the summ of elements that we have met before this element. So, for array \\[5,4,5,5,2,3,4,2,1\\] we get \\[1(1),2(2),1(3),2(4),3(5)\\] -> \\[0(1),1(2),3(3),4(4),6(5)\\]. The last array contains positions from where we should begin write elements. Now, when we got array with positions we can go throug original array and at every moment we meet element we look at position where it should be and write it in result array (don't forget make b\\[i\\]+=1 for current element 'i' to keep in memory the new position for the next 'i'). So, for example above we got $[0,0,0,0,0,0,5,0,0] -> [0,0,0,0,4,0,5,0,0] -> [0,0,0,0,4,0,5,5,0] -> [0,0,0,0,4,0,5,5,5] -> [0,2,0,0,4,0,5,5,5] -> [0,2,0,3,4,0,5,5,5] -> [0,2,0,3,4,4,5,5,5] -> [0,2,2,3,4,4,5,5,5] -> [1,2,2,3,4,4,5,5,5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we got stable counting sort we can make another one sort with complexity O(N*k) where k - the length of numbers (in digits) in array. It is called radix sort. So, the main idea is that if the values of elements to big our counting array will be too long. To prevent it instead of counting numbers we will be counting the digits in number. Mean firstly we sort array by first (left one) digit of the numbers. Next, sort by second one and etc. The size of counting array become exactly 10 elements (1,2,3,4,5,6,7,8,9,0) and work with it become much simpler. But will the sorting algorithm work when it sorting by numbers? And here we remember the stabel cunting sort. Because of stability the sorting of new digits not brake the previous one and if we have two numbers with the same current digits they still stay sorted by previous digits.\n",
    "\n",
    "![radix](images\\radixSort.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now about asymptotics and stability. I think that is clear that this algorithm is stable (because it consists of stable sorts). But what about complexity. Again we have division into steps and the steps number directly connect to digits amount inside the biggest number. Call this number K. Plus at every step we use stable counting sort that take O(10 + N) ~ O(N). In sum we get `O(N*K)` and `O(N)` memory for counting sort. For not big numbers it quite good as I think (at least it much better than basic counting sort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_sort(arr, max_value, get_index):\n",
    "  counts = [0] * max_value\n",
    "\n",
    "  # Counting - O(n)\n",
    "  for a in arr:\n",
    "    counts[get_index(a)] += 1\n",
    "  \n",
    "  # Accumulating - O(k)\n",
    "  for i, c in enumerate(counts):\n",
    "    if i == 0:\n",
    "      continue\n",
    "    else:\n",
    "      counts[i] += counts[i-1]\n",
    "\n",
    "  # Calculating start index - O(k)\n",
    "  for i, c in enumerate(counts[:-1]):\n",
    "    if i == 0:\n",
    "      counts[i] = 0\n",
    "    counts[i+1] = c\n",
    "\n",
    "  ret = [None] * len(arr)\n",
    "  # Sorting - O(n)\n",
    "  for a in arr:\n",
    "    index = counts[get_index(a)]\n",
    "    ret[index] = a\n",
    "    counts[get_index(a)] += 1\n",
    "  \n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digit(n, d):\n",
    "  for i in range(d-1):\n",
    "    n //= 10\n",
    "  return n % 10\n",
    "\n",
    "def get_num_difit(n):\n",
    "  i = 0\n",
    "  while n > 0:\n",
    "    n //= 10\n",
    "    i += 1\n",
    "  return i\n",
    "\n",
    "def radix_sort(arr):\n",
    "    max_value = max(arr)\n",
    "    num_digits = get_num_difit(max_value)\n",
    "    # O(k(n+k))\n",
    "    for d in range(num_digits):\n",
    "        # Counting sort takes O(n+k)\n",
    "        arr = counting_sort(arr, max_value, lambda a: get_digit(a, d+1))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 7, 7, 24, 34, 45, 45, 45, 46, 53, 64, 345, 346, 3554]\n"
     ]
    }
   ],
   "source": [
    "a = [53,45,64,7,34,3554,6,7,345,24,45,45,346,46,4]\n",
    "a = radix_sort(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heap sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last but not the least algorithm is heap sort. Main idea - lets make the tree, that can give us the biggest (smallest) element with O(logN) complexity. It is called the heap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heap\n",
    "\n",
    "Heap - the binary tree type data structure. At the root it has a biggest or smallest element, and every child element is less (bigger) than parent element. One more condition - the depths of all \"end\" elements (list) most not differ more than 1.\n",
    "\n",
    "![heap](images/heap.png)\n",
    "\n",
    "Ok, first of all we need build it. At the input we have a regular array. And we need to store somewhere the heap. The first thing that comes to mind is to create structure 'treeElement' and keep inside it links to childs element, to parent element and data. It's a good thought but structure is too 'heavy' in terms of overhead. So there is a simpler way - use usual array. The main point is we need create the rule where child and parent elements should placed to know about each other. So, at every 'layer' of heap we have $2^k$ elements where k is number of layer from top (begining from 0) - $2^k = 1,2,4,8,...$. Lets divide array on such layers. To link from previous louer to next we need only multiple it's index by 2 for first child and make +1 to second child because childs will placed near each other. And one moment - beacuse our indexes always begin from 0 we modified our rule like: parent a\\[i\\] - childs a\\[i+1\\] and a\\[i+2\\].\n",
    "\n",
    "What am I talking about? Yep, we need to build the heap. Ok, now we have the place where we can store it. Lets make a heap with biggest element at the top. So, the main point is that every parent is bigger than child. But what if child bigger than the parent and all other elements stay at right places? We swap them to restore the heap rule. And what if now child element become less that its childs (even lower level)? Swap again. And we can do this till meet the case with the correct order of elements. Or till we meet the bottom. All that remains is to make sure that subtree is already a heap. To do this we just need make from subtree a heap. This mean that before we will work with element a\\[i\\] we need work with elements a\\[i+1\\] and a\\[i+1\\]. Ok, Let's make a function heapRepair (i) (or just hR(i)) that repair heap rule as we describe before. And call it for all elements from N/2 to 0. Because we start from bottom, the elements there already the heap (from one element). So when you call hR(N/2), hR(N/2-1) they just check that elements at the bottom not bigger than elements on layer above. In other world we just build the heap layer by layer from the bottom.\n",
    "\n",
    "Lets do this for some array with prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hR (array, i):\n",
    "    N = len(array)\n",
    "    if i >= N//2: return #integer division\n",
    "    a = array\n",
    "    largest = 0\n",
    "    \n",
    "    if (a[2*i+1] >= a[2*i+2]): largest = 2*i+1\n",
    "    else: largest = 2*i+2\n",
    "\n",
    "    if (a[i] < a[largest]):\n",
    "        debStr = \"call for elements \" + str(i) + \", \" + str(2*i+1) + \", \" +str(2*i+2) + \" \" + str(array)\n",
    "        a[i], a[largest] = a[largest], a[i]\n",
    "        print (debStr + \" -> \" + str(array))\n",
    "        hR (array, largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heapSize = 7\n",
      "[1, 54, 7, 4, 36, 9, -inf]\n"
     ]
    }
   ],
   "source": [
    "array = [1,54,7,4,36,9]\n",
    "#array = [1,54,58,46,12,78,56,28,98,1,48,59,74,65,28,91]\n",
    "heapSize = 0\n",
    "for i in range(len(array)):\n",
    "    if len(array) <= heapSize: break\n",
    "    heapSize = heapSize + 2**i\n",
    "print (\"heapSize = \" + str(heapSize))\n",
    "array = array + [float('-inf')]*(heapSize - len(array))\n",
    "print (array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call for elements 2, 5, 6 [1, 54, 7, 4, 36, 9, -inf] -> [1, 54, 9, 4, 36, 7, -inf]\n",
      "call for elements 0, 1, 2 [1, 54, 9, 4, 36, 7, -inf] -> [54, 1, 9, 4, 36, 7, -inf]\n",
      "call for elements 1, 3, 4 [54, 1, 9, 4, 36, 7, -inf] -> [54, 36, 9, 4, 1, 7, -inf]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(array)//2-1, -1, -1):\n",
    "    hR (array, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example above we get:   \n",
    ">                                           54\n",
    ">                                          /  \\\n",
    ">                                         36   9\n",
    ">                                        / \\  / \\\n",
    ">                                       4  1  7  -inf\n",
    "\n",
    "Look correct. In addition we can add elements to heap, take a biggest (smallest) element, change elements inside it. To add element we add to the end of heap (-inf elements mean there are not exists, to understand that there are not elements we save the heap size) 'empty element' and change it (actualy we change -inf or resize array where heap is stored) to value of new one. To change element we need to understand, if it less than previous we change value and run hR to it. If it bigger than previous we need 'raise' it utill the parent less than it. For example if we add 10 in example above we need it raise to 9 (swap them), and after that we compare it with 54 and see, that 10 less than 54, so we stop here. In final 10 become third element in heap and 9 lower to the right.\n",
    "\n",
    "Finally to take a biggest element from the heap we need take the root, save it in variable, put to the heap root the last element from it and then run hR from root.\n",
    "\n",
    "The interface:\n",
    "0. hR(i) - hidden function to repare heap rules. Because we go through on bruch it will take no more than heap height. And the heap height is about log(N) because at every layer we take about 2^k elements from array.\n",
    "1. Contruct - at first sight it takes about O(NlogN) because N elements in array and logN operation for hR. But actually if we calculate it more carefully it will about O(N).\n",
    "2. Take biggest (smalest) element - O(1) to take the root and O(logN) for hR from root.\n",
    "3. Change element - O(1) to change the value and O(logN) for hR or reverse hR that we describe above.\n",
    "\n",
    "And now let sort the array!\n",
    "\n",
    "### Heap sort\n",
    "\n",
    "I think you have already guessed how to sort array with the heap. After we build it we just need to take the biggest element from heap and write it to the end of it (the heap will be think that this element is empty). And then take another one biggest element and write it to the end of memeory (before the last one). And take the another and another and another until the heap is empty. And we get a sorted array\n",
    "\n",
    "![heapSort](images/heapSort.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hRwithSize (array, size, i):\n",
    "    N = len(array)\n",
    "    if i >= N//2: return #integer division\n",
    "    a = array\n",
    "    largest = 0\n",
    "    \n",
    "    if 2*i+1 > size - 1: largest = i\n",
    "    elif 2*i+2 > size - 1: largest = 2*i+1\n",
    "    elif (a[2*i+1] >= a[2*i+2]): largest = 2*i+1\n",
    "    else: largest = 2*i+2\n",
    "\n",
    "    if (a[i] < a[largest]):\n",
    "        debStr = \"call for elements with size \" + str(size) + \" \" + str(i) + \", \" + str(2*i+1) + \", \" +str(2*i+2) + \" \" + str(array)\n",
    "        a[i], a[largest] = a[largest], a[i]\n",
    "        print (debStr + \" -> \" + str(array))\n",
    "        hRwithSize (array, size, largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeBiggest (array, size):\n",
    "    if size == 1: return array[0]\n",
    "    biggest = array[0]\n",
    "    array[0] = array [size-1]\n",
    "    hRwithSize(array, size, 0)\n",
    "    return biggest, size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapSort (array):\n",
    "    size = 0\n",
    "    for i in array:\n",
    "        if i == float('-inf'):break\n",
    "        size = size + 1\n",
    "\n",
    "    for i in range (size-1, 0, -1):\n",
    "        temp = size - 1\n",
    "        array[temp], size = takeBiggest (array, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call for elements with size 6 0, 1, 2 [7, 36, 9, 4, 1, 7, -inf] -> [36, 7, 9, 4, 1, 7, -inf]\n",
      "call for elements with size 5 0, 1, 2 [1, 7, 9, 4, 1, 54, -inf] -> [9, 7, 1, 4, 1, 54, -inf]\n",
      "call for elements with size 4 0, 1, 2 [4, 7, 1, 4, 36, 54, -inf] -> [7, 4, 1, 4, 36, 54, -inf]\n",
      "call for elements with size 3 0, 1, 2 [1, 4, 1, 9, 36, 54, -inf] -> [4, 1, 1, 9, 36, 54, -inf]\n",
      "[1, 4, 7, 9, 36, 54, -inf]\n"
     ]
    }
   ],
   "source": [
    "heapSort(array)\n",
    "print (array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability: The heap sort contain two parts: build the heap and taking the largest elements. Lets take a look to first one. And we already have a problem because equal elements can be in diffrent branches far away from each other. For example let's take two equal elements. The left branch from the root is already a heap and our element lies at the bottom. The second element lies at bottom too at the end of the heap, but after heapRepair it rose to child of the root and in the same time now it lies closer to the beginig of array that first one. But in original array it was at the end. Unfortunately this mean that heap sort is not stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So, in this lesson we see some of sorting algorithm. I hope that understanding the principles of their work will help you in your future work. Lets conclude our knowlidges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Algorithm     ...|       Worst   |Average   |Best    | Memory |Stability\n",
    "|---|---|---|---|\n",
    "|Bubble        |O(N^2)            |O(N^2)    |O(N)    |O(1)    | Yes\n",
    "|Merge         |O(NlogN)          |O(NlogN)  |O(NlogN)|O(N)    | Yes\n",
    "|QSort         |O(N^2)            |O(NlogN)  |O(NlogN)|O(1)    | No\n",
    "|Radix         |O(N*k)   |O(N*k)   |O(N*k)   | O(N) | Yes\n",
    "|Heap   |O(NlogN)   |O(NlogN)   |O(NlogN)   | O(1) | No"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
